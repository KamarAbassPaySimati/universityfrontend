trigger:
  branches:
    include:
      - qa
      - pre-production
      - master
pr:
  branches:
    include:
      - qa
variables:
  - group: general
  - ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/master') }}:
    - group: master
  - ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/qa') }}:
    - group: qa
  - ${{ if eq(variables['Build.SourceBranch'], 'refs/heads/pre-production') }}:
    - group: pre-production
  - ${{ if or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')) }}:
    - group: qa


stages:
- stage: CodeQuality
  jobs:
    - job: "lint"
      displayName: "Code Quality"
      condition: and(succeeded(), contains(variables['build.SourceBranch'], 'qa'))

      # condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')), succeeded())
      pool:
          vmImage: "ubuntu-latest"
      steps:
        - task: Cache@2
          inputs:
            key: 'npmV3 | "$(Agent.OS)" | package.json'
            restoreKeys: |
              npm | "$(Agent.OS)"
            path: "$(System.DefaultWorkingDirectory)/node_modules"
            cacheHitVar: CACHE_RESTORED
          displayName: Cache npm
        - script: |
            npm install --force
            node scripts/licences_complaince.js
            export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID_MAIN}
            export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY_MAIN}
            aws s3 cp licenses.json s3://$S3_BUCKET_LIVING_DOCUMENTATION/licences-ADMIN/dev/licenses.json
          condition: ne(variables.CACHE_RESTORED, 'true')
          displayName: 'Dependencies'
        - script: |
            npm run eslint
          displayName: 'Lint'
    - job: "secret"
      displayName: "Secret Checking"
      condition: and(succeeded(), contains(variables['build.SourceBranch'], 'qa'))
      # condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')), succeeded())
      pool:
          vmImage: "ubuntu-latest"
      steps:
        - script: |
            pip3 install detect-secrets
            detect-secrets scan
          displayName: 'Lint'
    - job: "licence_complaiance"
      displayName: "License Compliance"
      condition: and(succeeded(), contains(variables['build.SourceBranch'], 'qa'))
      # condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')), succeeded())
      pool:
          vmImage: "ubuntu-latest"
      steps:
        - script: |
            npm install -g license-checker 
            license-checker --json > licenses.json  
            node scripts/license.js
          displayName: 'License Compliance'
    - job: "sast"
      displayName: 'Static Application Security Testing (SAST)'
      condition: and(succeeded(), contains(variables['build.SourceBranch'], 'qa'))
      # condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')), succeeded())
      pool:
          vmImage: "ubuntu-latest"
      steps:
        - script: |
            pip3 install njsscan
            njsscan -w .
          displayName: 'Static Application Security Testing (SAST)'
- stage: Build
  jobs:
    - job: "Download_Environment"
      displayName: "Dependencies"
      pool:
        vmImage: "ubuntu-latest"
      steps:
        - script: |
            ./scripts/azure_envs.sh
          displayName: 'Copy Environment Files'
        - publish: $(System.DefaultWorkingDirectory)/.env
          artifact: Environment
    - job: build
      displayName: "Build"
      dependsOn: Download_Environment
      pool:
        vmImage: "ubuntu-latest"
      steps:
        - task: DownloadPipelineArtifact@2
          name: environment
          inputs:
            artifact: Environment
            targetPath: '$(System.DefaultWorkingDirectory)'
        - task: Cache@2
          inputs:
            key: 'npmV3 | "$(Agent.OS)" | package.json'
            restoreKeys: |
              npm | "$(Agent.OS)"
            path: "$(System.DefaultWorkingDirectory)/node_modules"
            cacheHitVar: CACHE_RESTORED
          displayName: Cache npm
        - script: |
            npm install --force
          condition: ne(variables.CACHE_RESTORED, 'true')
          displayName: 'Dependencies'
        - script: |
            node --version
            npm --version
            npm run build
            mkdir artifact
            mv dist artifact
          displayName: 'Build'
        - publish: $(System.DefaultWorkingDirectory)/artifact
          artifact: bundle
- stage: Test
  jobs:
    # - job: UnitTesting
    #   displayName: "Unit Testing"
    #   condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')), succeeded())
    #   pool:
    #     vmImage: "macos-latest"
    #   steps:
    #     - task: Cache@2
    #       inputs:
    #         key: 'npmV3 | "$(Agent.OS)" | package.json'
    #         restoreKeys: |
    #           npm | "$(Agent.OS)"
    #         path: "$(System.DefaultWorkingDirectory)/node_modules"
    #         cacheHitVar: CACHE_RESTORED
    #       displayName: Cache npm
    #     - script: |
    #         npm install --force
    #       condition: ne(variables.CACHE_RESTORED, 'true')
    #     - task: DownloadPipelineArtifact@2
    #       name: environment
    #       inputs:
    #         artifact: Environment
    #         targetPath: '$(System.DefaultWorkingDirectory)'
    #     - task: DownloadPipelineArtifact@2
    #       name: bundle
    #       inputs:
    #         artifact: bundle
    #         targetPath: '$(System.DefaultWorkingDirectory)'
    #     - script: |
    #         npm install pm2@5.3.0 -g
    #         source .env
    #         echo "$VITE_COVERAGE $VITE_REGION $VITE_ADMIN_COGNITO_USERPOOL_ID $VITE_ADMIN_COGNITO_CLIENT_ID $VITE_ADMIN_COGNITO_IDENTITY_POOL_ID $VITE_STAGE"
    #         echo "$VITE_BDD_TOKEN $VITE_DOMAIN_NAME $VITE_OTP_ENCODING $VITE_GOOGLE_API $VITE_BUCKET_NAME $VITE_CDN_URL"
    #         npm install express --force
    #         pm2 start scripts/server.js
    #         node --version
    #         sleep 5
    #         mkdir reports
    #         mkdir reports-xml
    #         npm install nyc --save-dev --force
    #         node scripts/bdd_unit_testing_azure.js
    #       displayName: Unit Testing
    #     - publish: $(System.DefaultWorkingDirectory)/test/step-definations/failedScenarios/ # Assuming logs are in working directory
    #       artifact: '$(system.JobId)-failedScenarios'
    #       condition: always()

    - job: UnitTesting1
      displayName: "Unit Testing-1"
      condition: and(succeeded(), contains(variables['build.SourceBranch'], 'qa'))
      # condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')), succeeded())
      pool:
        vmImage: "macos-latest"
      steps:
        - task: Cache@2
          inputs:
            key: 'npmV3 | "$(Agent.OS)" | package.json'
            restoreKeys: |
              npm | "$(Agent.OS)"
            path: "$(System.DefaultWorkingDirectory)/node_modules"
            cacheHitVar: CACHE_RESTORED
          displayName: Cache npm
        - script: |
            npm install --force
          condition: ne(variables.CACHE_RESTORED, 'true')
        - task: DownloadPipelineArtifact@2
          name: environment
          inputs:
            artifact: Environment
            targetPath: '$(System.DefaultWorkingDirectory)'
        - task: DownloadPipelineArtifact@2
          name: bundle
          inputs:
            artifact: bundle
            targetPath: '$(System.DefaultWorkingDirectory)'
        - script: |
            npm install pm2@5.3.0 -g
            source .env
            echo "$VITE_COVERAGE $VITE_REGION $VITE_ADMIN_COGNITO_USERPOOL_ID $VITE_ADMIN_COGNITO_CLIENT_ID $VITE_ADMIN_COGNITO_IDENTITY_POOL_ID $VITE_STAGE"
            echo "$VITE_BDD_TOKEN $VITE_DOMAIN_NAME $VITE_OTP_ENCODING $VITE_GOOGLE_API $VITE_BUCKET_NAME $VITE_CDN_URL"
            npm install express --force
            pm2 start scripts/server.js
            node --version
            sleep 5
            mkdir reports
            mkdir reports-xml
            npm install nyc --save-dev --force
            node scripts/bdd_unit_testing_azure1.js
            if [ $? -ne 0 ]; then
              echo "Integration Testing"
              exit 1
            fi
          displayName: Unit Testing
        - publish: $(System.DefaultWorkingDirectory)/test/step-definations/failedScenarios/ # Assuming logs are in working directory
          artifact: '$(system.JobId)-failedScenarios'
          condition: always()
        - publish: $(System.DefaultWorkingDirectory)/test/step-definations/coverageData/ # Assuming logs are in working directory
          artifact: '$(system.JobId)-coverageData'
          condition: always()


    - job: UnitTesting2
      displayName: "Unit Testing-2"
      condition: and(succeeded(), contains(variables['build.SourceBranch'], 'qa'))
      # condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')), succeeded())
      pool:
        vmImage: "macos-latest"
      steps:
        - task: Cache@2
          inputs:
            key: 'npmV3 | "$(Agent.OS)" | package.json'
            restoreKeys: |
              npm | "$(Agent.OS)"
            path: "$(System.DefaultWorkingDirectory)/node_modules"
            cacheHitVar: CACHE_RESTORED
          displayName: Cache npm
        - script: |
            npm install --force
          condition: ne(variables.CACHE_RESTORED, 'true')
        - task: DownloadPipelineArtifact@2
          name: environment
          inputs:
            artifact: Environment
            targetPath: '$(System.DefaultWorkingDirectory)'
        - task: DownloadPipelineArtifact@2
          name: bundle
          inputs:
            artifact: bundle
            targetPath: '$(System.DefaultWorkingDirectory)'
        - script: |
            npm install pm2@5.3.0 -g
            source .env
            echo "$VITE_COVERAGE $VITE_REGION $VITE_ADMIN_COGNITO_USERPOOL_ID $VITE_ADMIN_COGNITO_CLIENT_ID $VITE_ADMIN_COGNITO_IDENTITY_POOL_ID $VITE_STAGE"
            echo "$VITE_BDD_TOKEN $VITE_DOMAIN_NAME $VITE_OTP_ENCODING $VITE_GOOGLE_API $VITE_BUCKET_NAME $VITE_CDN_URL"
            npm install express --force
            pm2 start scripts/server.js
            node --version
            sleep 5
            mkdir reports
            mkdir reports-xml
            npm install nyc --save-dev --force
            node scripts/bdd_unit_testing_azure2.js
            if [ $? -ne 0 ]; then
              echo "Integration Testing"
              exit 1
            fi
          displayName: Unit Testing
        - publish: $(System.DefaultWorkingDirectory)/test/step-definations/failedScenarios/ # Assuming logs are in working directory
          artifact: '$(system.JobId)-failedScenarios'
          condition: always()
        - publish: $(System.DefaultWorkingDirectory)/test/step-definations/coverageData/ # Assuming logs are in working directory
          artifact: '$(system.JobId)-coverageData'
          condition: always()


    # - job: BDDReports
    #   displayName: 'BDD Living documentation'
    #   dependsOn:
    #       - UnitTesting1
    #       - UnitTesting2
    #   condition: or(contains(variables['Build.SourceBranch'], 'feature'), contains(variables['Build.SourceBranch'], 'bugfix'), contains(variables['Build.SourceBranch'], 'hotfix'))

    #   pool:
    #     vmImage: "ubuntu-latest"
    #   steps:
    #     - script: |
    #         echo "##vso[task.setvariable variable=RepositoryName]$BUILD_REPOSITORY_NAME"
    #         echo "##vso[task.setvariable variable=SourceBranch]$SourceBranch"
    #         source .env
    #         mkdir coverage
    #         npm install nyc --save-dev --force
    #         npm run generate_report
    #         npm run generate_report_cobertura
    #         ls -R reports-xml
    #         node scripts/livingdoc_azure.js $BUILD_REPOSITORY_NAME
    #         export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID_MAIN}
    #         export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY_MAIN}
    #         aws s3 sync living-documentation s3://$S3_BUCKET_LIVING_DOCUMENTATION/living-documentation-ADMIN/dev/ --acl public-read
    #         aws s3 sync coverage s3://$S3_BUCKET_LIVING_DOCUMENTATION/coverage-ADMIN/dev/ --acl public-read
    #         aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID_DOCS --paths "/*"
    #       displayName: 'BDD Living documentation'
    #       condition: always()
    #     - task: PublishTestResults@2
    #       condition: always()
    #       inputs:
    #         testResultsFormat: 'JUnit'
    #         testResultsFiles: '**/TEST-*.xml'
    #     - task: PublishCodeCoverageResults@1
    #       condition: always()
    #       inputs:
    #         codeCoverageTool: 'Cobertura'
    #         summaryFileLocation: 'coverage-cobertura/cobertura-coverage.xml'

    # - job: UnitTestingResult
    #   variables:
    #     myVariable: $[replace(variables['BUILD.SOURCEBRANCH'], 'refs/heads/', '')]
    #   displayName: "Unit Testing Result"
    #   condition: or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix'))
    #   dependsOn: UnitTesting
    #   pool:
    #     vmImage: "ubuntu-latest"
    #   steps:
    #   - task: Cache@2
    #     inputs:
    #       key: 'npmV3 | "$(Agent.OS)" | package.json'
    #       restoreKeys: |
    #         npm | "$(Agent.OS)"
    #       path: "$(System.DefaultWorkingDirectory)/node_modules"
    #       cacheHitVar: CACHE_RESTORED
    #     displayName: Cache npm
    #   - script: |
    #       npm install --cache .npm
    #     condition: ne(variables.CACHE_RESTORED, 'true')
    #   - script: |
    #      mkdir reports
    #      mkdir test/step-definations/coverageData/
    #   - task: DownloadPipelineArtifact@2
    #     name: reports
    #     inputs:
    #       artifact: reports
    #       targetPath: '$(System.DefaultWorkingDirectory)/reports'
    #   - task: DownloadPipelineArtifact@2
    #     name: coverage
    #     inputs:
    #       artifact: coverage
    #       targetPath: '$(System.DefaultWorkingDirectory)/test/step-definations/coverageData/'
    #   - script: |
    #       echo "##vso[task.setvariable variable=RepositoryName]$BUILD_REPOSITORY_NAME"
    #       echo "##vso[task.setvariable variable=SourceBranch]$SourceBranch"
    #       source .env
    #       mkdir coverage
    #       npm install nyc --save-dev --force
    #       npm run generate_report
    #       node scripts/livingdoc_azure.js $BUILD_REPOSITORY_NAME
    #       export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID_MAIN}
    #       export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY_MAIN}
    #       aws s3 sync living-documentation s3://$S3_BUCKET_LIVING_DOCUMENTATION/living-documentation-ADMIN/dev/ --acl public-read
    #       aws s3 sync coverage s3://$S3_BUCKET_LIVING_DOCUMENTATION/coverage-ADMIN/dev/ --acl public-read
    #       aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID_DOCS --paths "/*"
    #     displayName: 'BDD Living documentation'
    
    - job: IntegrationTesting
      displayName: "Integration Testing"
      # condition: and(succeeded(), contains(variables['build.SourceBranch'], 'qa'))
      condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix')), succeeded())
      pool:
        vmImage: "macos-latest"
      steps:
        - task: Cache@2
          inputs:
            key: 'npmV3 | "$(Agent.OS)" | package.json'
            restoreKeys: |
              npm | "$(Agent.OS)"
            path: "$(System.DefaultWorkingDirectory)/node_modules"
            cacheHitVar: CACHE_RESTORED
          displayName: Cache npm
        - script: |
            npm install --force
          condition: ne(variables.CACHE_RESTORED, 'true')
        - task: DownloadPipelineArtifact@2
          name: environment
          inputs:
            artifact: Environment
            targetPath: '$(System.DefaultWorkingDirectory)'
        - task: DownloadPipelineArtifact@2
          name: bundle
          inputs:
            artifact: bundle
            targetPath: '$(System.DefaultWorkingDirectory)'
        - script: |
            npm install pm2@5.3.0 -g
            source .env
            export NODE_OPTIONS=--max-old-space-size=8192
            npm install express --force
            pm2 start scripts/server.js
            sleep 60
            mkdir reports
            npm install nyc --save-dev --force
            mkdir reports-xml
            node scripts/bdd-execution-split1.js
            if [ $? -ne 0 ]; then
              echo "Integration Testing"
              exit 1
            fi
          displayName: Integration Testing
        - script: |
            cat log.txt
          condition: always()
        - script: |
            mkdir coverage
            npm run generate_report
            # ls -R reports-xml
            node scripts/livingdoc_azure.js $BUILD_REPOSITORY_NAME
            export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID_MAIN}
            export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY_MAIN}
            aws s3 sync living-documentation s3://$S3_BUCKET_LIVING_DOCUMENTATION/living-documentation-ADMIN/dev
            aws s3 sync coverage s3://$S3_BUCKET_LIVING_DOCUMENTATION/coverage-ADMIN/dev
            aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID_DOCS --paths "/*"
            if [ $? -ne 0 ]; then
              echo "BDD Living documentation"
              exit 1
            fi
          displayName: BDD Living documentation
          condition: always()
        - publish: $(System.DefaultWorkingDirectory)/test/step-definations/failedScenarios/ # Assuming logs are in working directory
          artifact: '$(system.JobId)-failedScenarios'
          condition: always()
        - publish: $(System.DefaultWorkingDirectory)/test/step-definations/coverageData/ # Assuming logs are in working directory
          artifact: '$(system.JobId)-coverageData'
          condition: always()
        - publish: $(System.DefaultWorkingDirectory)/reports/ # Assuming logs are in working directory
          artifact: '$(system.JobId)-living-documentation'
          condition: always()

        # - task: PublishTestResults@2
        #   condition: always()
        #   inputs:
        #     testResultsFormat: 'JUnit'
        #     testResultsFiles: '**/TEST-*.xml'

  

    - job: BrowserPerformanceTesting
      displayName: "Browser Performance Testing"
      condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix'), contains(variables['build.SourceBranch'], 'qa')), succeeded())
      pool:
        vmImage: "ubuntu-latest"
      steps:
        - task: Cache@2
          inputs:
            key: 'npmV2 | "$(Agent.OS)" | package.json'
            restoreKeys: |
              npm | "$(Agent.OS)"
            path: "$(System.DefaultWorkingDirectory)/node_modules"
            cacheHitVar: CACHE_RESTORED
          displayName: Cache npm
        - script: |
            npm install --force
          condition: ne(variables.CACHE_RESTORED, 'true')
        - task: DownloadPipelineArtifact@2
          name: environment
          inputs:
            artifact: Environment
            targetPath: '$(System.DefaultWorkingDirectory)'
        - task: DownloadPipelineArtifact@2
          name: bundle
          inputs:
            artifact: bundle
            targetPath: '$(System.DefaultWorkingDirectory)'
        - script: |
            source .env
            npm install express --force
            npm install pm2 sitespeed.io -g
            pm2 start scripts/sitespeed-server.js
            sleep 30
            sitespeed.io  --multi scripts/sitespeed/testMultipleURLs.js --preScript scripts/sitespeed/login.js --budget.configPath scripts/budget.json -n 1 --firstParty ".amazon.com" --firstParty "sdk.relicx.ai" --headless true --browsertime.chrome.args no-sandbox
          displayName: BrowserPerformance
        - publish: $(System.DefaultWorkingDirectory)/sitespeed-result # Assuming logs are in working directory
          artifact: sitespeed-result
          condition: always()
     
- stage: Deploy
  jobs:
    - deployment: Deployment
      condition: succeeded()
      displayName: Deployment
      pool:
        vmImage: 'macos-latest'
      ${{ if eq(variables['Build.SourceBranchName'], 'master') }}:
          environment: 'Production'
      ${{ elseif eq(variables['Build.SourceBranchName'], 'qa') }}:
          environment: 'Testing'
      ${{ else }}:
          environment: 'Development'
      strategy:
        # Default deployment strategy, more coming...
        runOnce:
          deploy:
            steps:
              - task: DownloadPipelineArtifact@2
                name: bundle
                inputs:
                  artifact: bundle
                  targetPath: '$(System.DefaultWorkingDirectory)'
              - task: DownloadPipelineArtifact@2
                name: environment
                inputs:
                  artifact: environment
                  targetPath: '$(System.DefaultWorkingDirectory)'
              - script: |
                  source .env
                  aws s3 sync dist s3://$ADMIN_S3_BUCKET --acl public-read
                  aws cloudfront create-invalidation --distribution-id $ADMIN_DISTRIBUTION_ID --paths "/*" 
                displayName: 'Deployment'
    - job: DAST
      displayName: "Dynamic Application Security Testing (DAST)"
      condition: and(or(contains(variables['build.SourceBranch'], 'feature'), contains(variables['build.SourceBranch'], 'bugfix'), contains(variables['build.SourceBranch'], 'hotfix'), contains(variables['build.SourceBranch'], 'qa')), succeeded())
      pool:
        vmImage: "ubuntu-latest"
      steps:
        - task: Cache@2
          inputs:
            key: 'npmV2 | "$(Agent.OS)" | package.json'
            restoreKeys: |
              npm | "$(Agent.OS)"
            path: "$(System.DefaultWorkingDirectory)/node_modules"
            cacheHitVar: CACHE_RESTORED
          displayName: Cache npm
        - script: |
            npm install --force
          condition: ne(variables.CACHE_RESTORED, 'true')
        - task: DownloadPipelineArtifact@2
          name: environment
          inputs:
            artifact: Environment
            targetPath: '$(System.DefaultWorkingDirectory)'
        - task: DownloadPipelineArtifact@2
          name: bundle
          inputs:
            artifact: bundle
            targetPath: '$(System.DefaultWorkingDirectory)'
        - script: |
            npm install pm2 -g
            npm install express --force
            pm2 start scripts/server.js
            docker run -u zap -p 8080:8080 -d softwaresecurityproject/zap-stable zap.sh -daemon -host 0.0.0.0 -port 8080 -config api.disablekey=true -config api.addrs.addr.name=.* -config api.addrs.addr.regex=true
            sleep 30
            mkdir test_results
            pip install beautifulsoup4
            pip install zaproxy
            python scripts/dast.py
          displayName: Dynamic Application Security Testing (DAST)
        - publish: $(System.DefaultWorkingDirectory)/test_results/report0.html # Assuming logs are in working directory
          artifact: dast_test_results
          condition: always()
        - script: |
            export AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID_MAIN}
            export AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY_MAIN}
            aws s3 sync living-documentation s3://$S3_BUCKET_LIVING_DOCUMENTATION/dast-admin/dev/ --acl public-read
            aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID_DOCS --paths "/*"
          displayName: 'DAST Report documentation'
          condition: always()
    
    
